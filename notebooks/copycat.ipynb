{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/brendalf/Documents/projects/gan-attack/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor, RandomCrop, RandomHorizontalFlip\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target.train import train_network\n",
    "from target.evaluate import evaluate_network\n",
    "\n",
    "from target.custom import CustomNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz + nl, ngf*4, 4, 1, 0),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        x = torch.cat((input, label), dim=1).unsqueeze(2).unsqueeze(3)\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "                 \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=ndf * 4 * 4 * 4,\n",
    "                out_features=nl,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.police = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(ndf * 4, 1, 4, 1, 0),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        features = self.main(input)\n",
    "        valid = self.police(features).view(-1, 1)\n",
    "        clf = self.clf(features.view(features.shape[0], -1))\n",
    "        return valid, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = torch.load('../models/adversary/g_cifar10_500img_modeseeking.pth').to(device)\n",
    "netD = torch.load('../models/adversary/d_cifar10_500img_modeseeking.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "training_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeOneHot(labels):\n",
    "    ret = torch.FloatTensor(labels.shape[0], num_classes)\n",
    "    ret.zero_()\n",
    "    ret.scatter_(dim=1, index=labels.view(-1, 1), value=1)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images from class 0\n",
      "Generating images from class 1\n",
      "Generating images from class 2\n",
      "Generating images from class 3\n",
      "Generating images from class 4\n",
      "Generating images from class 5\n",
      "Generating images from class 6\n",
      "Generating images from class 7\n",
      "Generating images from class 8\n",
      "Generating images from class 9\n"
     ]
    }
   ],
   "source": [
    "for n in np.arange(0, num_classes):\n",
    "    print(f\"Generating images from class {n}\")\n",
    "    output_path = os.path.join('../data/copycat_gan_vgg19/', str(n))\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    \n",
    "    for i in np.arange(0, 10):\n",
    "        X = torch.randn(training_size, 100).to(device)\n",
    "        y = encodeOneHot(\n",
    "            torch.ones(training_size, dtype=torch.int64) * n\n",
    "        ).to(device)\n",
    "        \n",
    "        generated = netG(X, y)\n",
    "        \n",
    "        for id, image in enumerate(generated):\n",
    "            new_id = (training_size * i) + id\n",
    "            torchvision.utils.save_image(\n",
    "                image,\n",
    "                fp=os.path.join(output_path, f'{new_id}.png')    \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagefolder = torchvision.datasets.ImageFolder(\n",
    "    '../data/copycat_gan_vgg19/',\n",
    "    transform=Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    imagefolder,\n",
    "    batch_size=16,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VGG('VGG19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"../models/adversary/copycat.vgg19.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_test = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 1/50 [train_loss: 1995.153, val_loss: 52412.984, val_acc: 10.000]\n",
      "E: 2/50 [train_loss: 1545.293, val_loss: 91899.405, val_acc: 10.000]\n",
      "E: 3/50 [train_loss: 1445.294, val_loss: 73201.231, val_acc: 10.000]\n",
      "E: 4/50 [train_loss: 1449.042, val_loss: 46025.673, val_acc: 10.000]\n",
      "E: 5/50 [train_loss: 1390.694, val_loss: 5162.335, val_acc: 10.000]\n",
      "E: 6/50 [train_loss: 1320.643, val_loss: 6369.206, val_acc: 10.000]\n",
      "E: 7/50 [train_loss: 1765.901, val_loss: 8417.816, val_acc: 10.000]\n",
      "E: 8/50 [train_loss: 1499.738, val_loss: 3062.364, val_acc: 10.000]\n",
      "E: 9/50 [train_loss: 1290.079, val_loss: 3652.005, val_acc: 10.000]\n",
      "E: 10/50 [train_loss: 1141.796, val_loss: 4182.205, val_acc: 10.000]\n",
      "E: 11/50 [train_loss: 1208.060, val_loss: 4060.819, val_acc: 10.000]\n",
      "E: 12/50 [train_loss: 1039.326, val_loss: 8480.868, val_acc: 10.000]\n",
      "E: 13/50 [train_loss: 1067.935, val_loss: 4172.867, val_acc: 10.000]\n",
      "E: 14/50 [train_loss: 977.466, val_loss: 2522.373, val_acc: 10.000]\n",
      "E: 15/50 [train_loss: 1189.932, val_loss: 2541.934, val_acc: 10.000]\n",
      "E: 16/50 [train_loss: 931.429, val_loss: 13064.330, val_acc: 10.000]\n",
      "E: 17/50 [train_loss: 1014.999, val_loss: 9576.119, val_acc: 10.000]\n",
      "E: 18/50 [train_loss: 921.783, val_loss: 4652.371, val_acc: 10.000]\n",
      "E: 19/50 [train_loss: 1141.588, val_loss: 7950.913, val_acc: 10.000]\n",
      "E: 20/50 [train_loss: 826.443, val_loss: 7559.631, val_acc: 10.000]\n",
      "E: 21/50 [train_loss: 822.272, val_loss: 2094.863, val_acc: 10.000]\n",
      "E: 22/50 [train_loss: 870.093, val_loss: 1725.771, val_acc: 10.000]\n",
      "E: 23/50 [train_loss: 872.732, val_loss: 1773.724, val_acc: 10.000]\n",
      "E: 24/50 [train_loss: 1157.549, val_loss: 2798.456, val_acc: 9.740]\n",
      "E: 25/50 [train_loss: 1050.727, val_loss: 1509.018, val_acc: 10.000]\n",
      "E: 26/50 [train_loss: 1146.271, val_loss: 1850.615, val_acc: 10.000]\n",
      "E: 27/50 [train_loss: 928.462, val_loss: 1621.672, val_acc: 10.000]\n",
      "E: 28/50 [train_loss: 1031.428, val_loss: 1716.333, val_acc: 10.000]\n",
      "E: 29/50 [train_loss: 1088.980, val_loss: 1681.446, val_acc: 10.000]\n",
      "E: 30/50 [train_loss: 922.496, val_loss: 1710.827, val_acc: 10.000]\n",
      "E: 31/50 [train_loss: 770.585, val_loss: 2349.926, val_acc: 10.000]\n",
      "E: 32/50 [train_loss: 606.208, val_loss: 2239.389, val_acc: 10.000]\n",
      "E: 33/50 [train_loss: 572.462, val_loss: 1950.629, val_acc: 10.040]\n",
      "E: 34/50 [train_loss: 525.656, val_loss: 2081.108, val_acc: 9.970]\n",
      "E: 35/50 [train_loss: 497.482, val_loss: 1994.329, val_acc: 10.170]\n",
      "E: 36/50 [train_loss: 521.862, val_loss: 2072.467, val_acc: 10.100]\n",
      "E: 37/50 [train_loss: 496.524, val_loss: 2219.308, val_acc: 11.160]\n",
      "E: 38/50 [train_loss: 504.092, val_loss: 1982.351, val_acc: 10.440]\n",
      "E: 39/50 [train_loss: 454.814, val_loss: 2433.779, val_acc: 11.140]\n",
      "E: 40/50 [train_loss: 403.760, val_loss: 2379.956, val_acc: 11.110]\n",
      "E: 41/50 [train_loss: 343.304, val_loss: 2359.381, val_acc: 10.930]\n",
      "E: 42/50 [train_loss: 305.144, val_loss: 2561.654, val_acc: 11.600]\n",
      "E: 43/50 [train_loss: 282.433, val_loss: 2901.466, val_acc: 11.760]\n",
      "E: 44/50 [train_loss: 253.114, val_loss: 3201.625, val_acc: 12.380]\n",
      "E: 45/50 [train_loss: 224.385, val_loss: 3614.090, val_acc: 12.700]\n",
      "E: 46/50 [train_loss: 207.653, val_loss: 4258.839, val_acc: 12.520]\n",
      "E: 47/50 [train_loss: 191.995, val_loss: 4947.463, val_acc: 12.590]\n",
      "E: 48/50 [train_loss: 184.803, val_loss: 5043.657, val_acc: 12.470]\n",
      "E: 49/50 [train_loss: 165.516, val_loss: 4932.165, val_acc: 12.480]\n",
      "E: 50/50 [train_loss: 149.434, val_loss: 4072.797, val_acc: 12.050]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#best device available\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad(): # turn off grad\n",
    "            model.eval() # network in evaluation mode\n",
    "\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = (100 * correct / total)\n",
    "    print('E: {}/{} [train_loss: {:.3f}, val_loss: {:.3f}, val_acc: {:.3f}]'.format(epoch+1, epochs, running_loss, val_loss, acc))\n",
    "    #print('E: {}/{} [train_loss: {:.3f}]'.format(epoch+1, epochs, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:03<00:00, 96.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 12 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.05"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_network(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
